{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFr_0ZzpuMfX",
        "outputId": "3c272623-46e9-44d9-845b-e6137f63b282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this line\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def detect_dizziness():\n",
        "    # Load the pre-trained face detector\n",
        "    face_cascade_path = 'haarcascade_frontalface_default.xml'\n",
        "    if not os.path.exists(face_cascade_path):\n",
        "        print(\"this line\")\n",
        "        return\n",
        "    face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
        "\n",
        "    #Load the pre-trained dizziness detection model\n",
        "    model_path = 'dizziness_detection_model.h5'\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"This line\")\n",
        "        return\n",
        "    dizziness_model = load_model(model_path)\n",
        "\n",
        "    # Open the video capture device\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    while True:\n",
        "        # Read a frame from the video capture device\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Convert the frame to grayscale\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces in the grayscale frame\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        # For each face, predict dizziness\n",
        "        for (x, y, w, h) in faces:\n",
        "            face_roi = gray[y:y+h, x:x+w]\n",
        "            face_roi = cv2.resize(face_roi, (96, 96))\n",
        "            face_roi = face_roi / 255.0\n",
        "            face_roi = np.expand_dims(face_roi, axis=0)\n",
        "            face_roi = np.expand_dims(face_roi, axis=-1)\n",
        "            dizziness = dizziness_model.predict(face_roi)[0][0]\n",
        "\n",
        "            # If the driver is dizzy, print a message and break out of the loop\n",
        "            if dizziness == 1:\n",
        "                print(\"Driver is dizzy!\")\n",
        "                break\n",
        "\n",
        "            # Draw a rectangle around the face\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # Display the frame\n",
        "        cv2.imshow(\"Driver Dizziness Detector\", frame)\n",
        "\n",
        "        # Wait for a key press\n",
        "        if cv2.waitKey(1) == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release the video capture device and close all windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Call the detect_dizziness function to start detecting dizziness\n",
        "detect_dizziness()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in tqdm(glob(\"/content/WhatsApp Image 2023-10-08 at 9.48.58 PM (1).jpeg\")):\n",
        "    temp = np.array(Image.open(i).resize((64,64)))\n",
        "    X.append(temp)\n",
        "    Y.append(1)\n",
        "\n",
        "for i in tqdm(glob(\"/content/WhatsApp Image 2023-10-08 at 9.49.28 PM (1).jpeg\")):\n",
        "    temp = np.array(Image.open(i).resize((64,64)))\n",
        "    X.append(temp)\n",
        "    Y.append(0)\n",
        "\n",
        "X = (np.array(X) - np.min(X)) / (np.max(X) - np.min(X))\n",
        "Y = (np.array(Y) - np.min(Y)) / (np.max(Y) - np.min(Y))\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "history = model.fit(datagen.flow(X_train, Y_train, batch_size=32),\n",
        "                    epochs=50,\n",
        "                    validation_data=(X_test, Y_test))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "model.save('dizziness_detection_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4dLNZHTeTIQ",
        "outputId": "c1698e9b-a888-4ec6-b527-79ba5284773c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 539.88it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 963.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step - loss: 0.6925 - accuracy: 1.0000 - val_loss: 0.8114 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.5817 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.3976 - accuracy: 1.0000 - val_loss: 1.4493 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.5138 - accuracy: 1.0000 - val_loss: 2.1591 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0697 - accuracy: 1.0000 - val_loss: 3.3330 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0768 - accuracy: 1.0000 - val_loss: 5.1522 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 7.3515 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 9.8955 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 3.1371e-05 - accuracy: 1.0000 - val_loss: 12.6531 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3548e-05 - accuracy: 1.0000 - val_loss: 15.6331 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 7.9820e-04 - accuracy: 1.0000 - val_loss: 18.7479 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 4.1951e-10 - accuracy: 1.0000 - val_loss: 21.9233 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 5.9484e-12 - accuracy: 1.0000 - val_loss: 25.1152 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.0731e-09 - accuracy: 1.0000 - val_loss: 28.2868 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 4.7522e-11 - accuracy: 1.0000 - val_loss: 31.4078 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 2.6840e-10 - accuracy: 1.0000 - val_loss: 34.4531 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 6.2090e-19 - accuracy: 1.0000 - val_loss: 37.4032 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.0864e-17 - accuracy: 1.0000 - val_loss: 40.2435 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3365e-09 - accuracy: 1.0000 - val_loss: 42.9633 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.2372e-20 - accuracy: 1.0000 - val_loss: 45.5549 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 3.3100e-17 - accuracy: 1.0000 - val_loss: 48.0137 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 4.6502e-14 - accuracy: 1.0000 - val_loss: 50.3374 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 6.3548e-16 - accuracy: 1.0000 - val_loss: 52.5263 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 5.5106e-25 - accuracy: 1.0000 - val_loss: 54.5816 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 4.2435e-20 - accuracy: 1.0000 - val_loss: 56.5061 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 3.5771e-27 - accuracy: 1.0000 - val_loss: 58.3035 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3326e-22 - accuracy: 1.0000 - val_loss: 59.9784 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.4802e-15 - accuracy: 1.0000 - val_loss: 61.5358 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5421e-31 - accuracy: 1.0000 - val_loss: 62.9812 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 6.0764e-13 - accuracy: 1.0000 - val_loss: 64.3202 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 7.4222e-29 - accuracy: 1.0000 - val_loss: 65.5589 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3193e-12 - accuracy: 1.0000 - val_loss: 66.7029 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 5.6271e-26 - accuracy: 1.0000 - val_loss: 67.7581 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 4.2056e-11 - accuracy: 1.0000 - val_loss: 68.7302 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.4552e-23 - accuracy: 1.0000 - val_loss: 69.6247 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 7.4968e-29 - accuracy: 1.0000 - val_loss: 70.4470 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.9202e-29 - accuracy: 1.0000 - val_loss: 71.2021 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 4.2689e-38 - accuracy: 1.0000 - val_loss: 71.8948 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 1.7885e-32 - accuracy: 1.0000 - val_loss: 72.5299 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 3.9815e-29 - accuracy: 1.0000 - val_loss: 73.1116 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 5.9011e-37 - accuracy: 1.0000 - val_loss: 73.6440 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 6.1271e-22 - accuracy: 1.0000 - val_loss: 74.1310 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 9.0716e-33 - accuracy: 1.0000 - val_loss: 74.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 1.3145e-38 - accuracy: 1.0000 - val_loss: 74.9828 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.8490e-29 - accuracy: 1.0000 - val_loss: 75.3541 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 3.2677e-23 - accuracy: 1.0000 - val_loss: 75.6931 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 7.1033e-26 - accuracy: 1.0000 - val_loss: 76.0022 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 9.8913e-23 - accuracy: 1.0000 - val_loss: 76.2840 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 2.4569e-11 - accuracy: 1.0000 - val_loss: 76.5409 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 8.6092e-28 - accuracy: 1.0000 - val_loss: 76.7750 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 76.7750 - accuracy: 0.0000e+00\n",
            "Test accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGyEIPrQohPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_vwurLtofqc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}